[
  {
    "objectID": "posts/clustered-barplots-r.html",
    "href": "posts/clustered-barplots-r.html",
    "title": "Creating clustered bar graphs with R base graphics",
    "section": "",
    "text": "A clustered bar graph is useful for comparing values across two categories of data. In this type of graph, we organize the bars in groups based on the levels of the first categorical variable. The height of each bar within a group represents the value of a level of the second categorical variable.\nYou can create this graph in R using the pre-installed base graphics package. This package has many plotting functions that are easy to use and excel at generating basic plots quickly. Use the command library(help = “graphics”) to get more information on this package, including a complete list of its functions."
  },
  {
    "objectID": "posts/clustered-barplots-r.html#pre-requisities",
    "href": "posts/clustered-barplots-r.html#pre-requisities",
    "title": "Creating clustered bar graphs with R base graphics",
    "section": "Pre-requisities",
    "text": "Pre-requisities\nTo follow through with the activities in this article, you should have a working knowledge of R."
  },
  {
    "objectID": "posts/clustered-barplots-r.html#the-data",
    "href": "posts/clustered-barplots-r.html#the-data",
    "title": "Creating clustered bar graphs with R base graphics",
    "section": "The Data",
    "text": "The Data\nConsider the sales dataset below.\n\n\nStore\nProduct\nSales ($)\n\n\n\nA\nP1\n42,500\n\n\nB\nP1\n48,502\n\n\nC\nP1\n39,500\n\n\nA\nP2\n43,600\n\n\nB\nP2\n32,303\n\n\nC\nP2\n29,500\n\n\nA\nP3\n49,270\n\n\nB\nP3\n49,664\n\n\nC\nP3\n48,507\n\n\n\nIn this dataset, Store and Product are categorical variables, while Sales is a numerical variable. You can use a clustered bar graph to compare the monthly sales of the three products across the three stores. The bars in this graph will be organized into three groups based on store levels, with each bar representing the sales figure for a specific product."
  },
  {
    "objectID": "posts/clustered-barplots-r.html#how-to-create-the-graph",
    "href": "posts/clustered-barplots-r.html#how-to-create-the-graph",
    "title": "Creating clustered bar graphs with R base graphics",
    "section": "How to create the graph",
    "text": "How to create the graph\nStep 1: Reorganize the dataset\nWe use the barplot() function in the base graphics package to plot the clustered bar graph. However, we must reorganize the dataset into a wide format to use this function effectively. In this format, each unique product occupies a single row, and each cell corresponds to a sales figure. Currently, the dataset has a long format, with store and product values spread across multiple rows.\nThe barplot() function requires the dataset to be in vector or matrix form. In this exercise, we input the data as a matrix using the matrix() function.\nHere is the code:\n\nmydata &lt;- matrix(c(42500, 48502, 39500, 43600, 32303, 29500, 49270, 49664, 48507), nrow = 3, byrow = TRUE)\nrownames(mydata) &lt;- c(\"P1\", \"P2\", \"P3\")\ncolnames(mydata) &lt;- c(\"A\", \"B\", \"C\")\nmydata\n\n       A     B     C\nP1 42500 48502 39500\nP2 43600 32303 29500\nP3 49270 49664 48507\n\n\nThe matrix() function has two crucial arguments, nrow and byrow. nrow specifies the number of rows in the matrix, while byrow, set to TRUE, indicates that the matrix is populated row by row.\nStep 2: Create the graph\nThe code provided below uses the barplot() function to create the graph shown in Figure 1.\n\nbarplot(mydata, beside = TRUE, xlab = \"Store\", ylab = \"Sales ($)\", legend.text = TRUE)\n\n\n\n\n\n\nFigure 1: Clustered bar graph created using barplot().\n\n\n\n\nAs demonstrated in the code, barplot() accepts several arguments. Use the command ?barplot to obtain a complete list of the function’s arguments.\nThe argument beside must be set to TRUE to ensure the bars within groups appear side-by-side, creating the desired clustered bar graph. If set to FALSE, the bars will appear stacked on each other. Additionally, we use the xlab and ylab arguments to specify the labels for the graph’s x-axis and y-axis, respectively.\nBy setting the argument legendtext to TRUE, we create a legend for the graph. A legend is a crucial element of a clustered bar graph because it helps users associate bar colors with distinct data points.\nStep 3: Customize the graph\nThe graph in Figure 1 is a basic clustered bar graph with few elements. We can enhance the graph’s clarity by adding colors, customizing the legend, adjusting the margins, and rescaling the y-axis.\n1. Add colors\nWe use the Pastel1 palette from the RColorBrewer package to color the bars of the graph based on product levels.\nIf you don’t already have this package installed, do so with the command install.packages(\"RColorBrewer\").\nAfter installation, use the command library(RColorBrewer) to load the package.\nOnce loaded, you can access the brewer.pal() function and use it to create a vector of colors for your graph. This function takes two arguments: the number of colors and the name of the color palette. For a complete list of the palettes in Rcolorbrewer, use the command display.brewer.all().\nWe use the following code to add colors and create the graph shown in Figure 2.\n\nlibrary(RColorBrewer)\nbarplot(mydata, beside = TRUE, col = brewer.pal(3, name = \"Pastel1\"), xlab = \"Store\", ylab = \"Sales ($)\", legend.text = TRUE)\n\n\n\n\n\n\nFigure 2: Clustered bar graph with added product colors.\n\n\n\n\n2. Customize the legend\nWe can enhance the legend of the graph in Figure 2 by\n\nRepositioning it to avoid blocking the bars.\nRemoving the box around it.\nAdding a title.\n\nFor these adjustments, we pass the following arguments to args.legend() as a list:\n\ntitle = “Product”: sets the title of the legend.\nx = “right”: positions the legend to the right of the graph.\nbty = “n”: suppresses the box around the legend, reducing clutter.\ninset = c(-0.16): shifts the legend 0.16 units to the right.\n\nThe adjustments result in the graph shown in Figure 3.\n\nbarplot(mydata, beside = TRUE, col = brewer.pal(3, name = \"Pastel1\"), xlab = \"Store\", ylab = \"Sales ($)\", legend.text = TRUE, args.legend = list(title = \"Product\", x = \"right\", bty = \"n\", inset = -0.16))\n\n\n\n\n\n\nFigure 3: Clustered bar graph with customized legend.\n\n\n\n\n3. Adjust the margins\nFigure 3 shows that the right margin lacks sufficient space for the legend. To resolve this problem, we increase the size of the right margin using the par() function with the mai argument, specifying the margin in inches. Use the command par(\"mai\") to check the plot’s default margin settings.\nAdditionally, we can use the space argument to reduce the space between the groups of bars, thus creating more room in the right margin. This argument accepts two values. The first value indicates the space between bars within a group, while the second value denotes the space between groups of bars. These changes result in the graph in Figure 4.\n\npar(mai=c(1, 1, 1, 1.2))\nbarplot(mydata, beside = TRUE, col = brewer.pal(3, name = \"Pastel1\"), space = c(0, 0.6), xlab = \"Store\", ylab = \"Sales ($)\", legend.text = TRUE, args.legend = list(title = \"Product\", x = \"right\", bty = \"n\", inset = -0.16))\n\n\n\n\n\n\nFigure 4: Clustered bar graph with adjusted margins.\n\n\n\n\n4. Rescale the y-axis\nWe use the ylim argument to specify the range of the y-axis. This argument accepts a vector containing the smallest and largest values on the y-axis. Here is the code used to create the graph shown in Figure 5.\n\npar(mai=c(1, 1, 1, 1.2))\nbarplot(mydata, beside = TRUE, col = brewer.pal(3, name = \"Pastel1\"), space = c(0, 0.6), xlab = \"Store\", ylab = \"Sales ($)\", ylim = c(0, 60000), legend.text = TRUE, args.legend = list(title = \"Product\", x = \"right\", bty = \"n\", inset = -0.16))\n\n\n\n\n\n\nFigure 5: Clustered bar graph with various modifications."
  },
  {
    "objectID": "posts/clustered-barplots-r.html#conclusion",
    "href": "posts/clustered-barplots-r.html#conclusion",
    "title": "Creating clustered bar graphs with R base graphics",
    "section": "Conclusion",
    "text": "Conclusion\nThis article has shown that you can create a visually appealing and effective clustered bar graph using the barplot() function from R’s base graphics package. The function has several arguments that allow you to customize the graph’s appearance to meet your needs. While graphing, remember that simplicity and clarity are crucial to effectively communicating your data to viewers."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Hi, I’m Ruth.",
    "section": "",
    "text": "Welcome to my digital workspace!\nI’m a data analyst passionate about turning messy data into insightful stories. My work blends statistics, machine learning, and visual storytelling to address real-world problems.\nI come from a strong background in mathematics and education, with over a decade spent making complex concepts make sense. These days, I channel that clarity into crafting analytical solutions and writing articles that simplify mathematical topics."
  },
  {
    "objectID": "index.html#what-youll-find-here",
    "href": "index.html#what-youll-find-here",
    "title": "Hi, I’m Ruth.",
    "section": "What You’ll Find Here",
    "text": "What You’ll Find Here\nThis site is a curated collection of my projects, experiments, and thoughts on data analytics, along with a discussion of some elementary math ideas from my teaching days. Here are some of the things I share:\n\nData analysis tutorials\n\nMachine learning projects\n\nData visualizations\n\nHead over to the Blog page to dive in."
  },
  {
    "objectID": "index.html#lets-connect",
    "href": "index.html#lets-connect",
    "title": "Hi, I’m Ruth.",
    "section": "Let’s Connect",
    "text": "Let’s Connect\nIf you’re looking to collaborate or share ideas, you can find me here:\n\n💼 LinkedIn\n💻 GitHub"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Creating clustered bar graphs with Python\n\n\n\n\n\n\nData Visualization\n\n\nPython\n\n\n\n\n\n\n\n\n\nMar 19, 2025\n\n\nbarplots,Python\n\n\n\n\n\n\n\nCreating clustered bar graphs with R base graphics\n\n\n\n\n\n\nData Visualization\n\n\nR\n\n\n\n\n\n\n\n\n\nMar 19, 2025\n\n\nbarplots,R\n\n\n\n\n\n\n\nUsing logistic regression to predict income levels\n\n\n\n\n\n\nMachine Learning\n\n\nR\n\n\n\n\n\n\n\n\n\nSep 7, 2024\n\n\nlogistic regression,R\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/clustered-barplots-python.html",
    "href": "posts/clustered-barplots-python.html",
    "title": "Creating clustered bar graphs with Python",
    "section": "",
    "text": "A clustered bar graph, also known as a grouped bar chart, is used to plot numerical values for the levels of two categorical variables in a dataset. In these graphs, the bars are grouped based on the levels of one categorical variable. The colors of the bars within each group represent the levels of the second categorical variable. In this article, we use a dummy dataset to create clustered bar graphs with Python."
  },
  {
    "objectID": "posts/clustered-barplots-python.html#pre-requisites",
    "href": "posts/clustered-barplots-python.html#pre-requisites",
    "title": "Creating clustered bar graphs with Python",
    "section": "Pre-requisites",
    "text": "Pre-requisites\nTo follow through with the exercises in this article, you should\n\nHave a working knowledge of Python.\nHave the following Python libraries installed in your machine: pandas, matplotlib, plotly, and seaborn."
  },
  {
    "objectID": "posts/clustered-barplots-python.html#import-libraries",
    "href": "posts/clustered-barplots-python.html#import-libraries",
    "title": "Creating clustered bar graphs with Python",
    "section": "Import Libraries",
    "text": "Import Libraries\nImport all libraries necessary to complete this exercise using the following code:\n\n# Import libraries\nimport pandas as pd\nimport numpy as np\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport matplotlib.pyplot as plt\nimport seaborn as sns"
  },
  {
    "objectID": "posts/clustered-barplots-python.html#the-data",
    "href": "posts/clustered-barplots-python.html#the-data",
    "title": "Creating clustered bar graphs with Python",
    "section": "The Data",
    "text": "The Data\nConsider the dummy sales dataset below.\n\n\n\nStore\nProduct\nSales ($)\n\n\n\n\nA\nP1\n42,500\n\n\nB\nP1\n48,502\n\n\nC\nP1\n39,500\n\n\nA\nP2\n43,600\n\n\nB\nP2\n32,303\n\n\nC\nP2\n29,500\n\n\nA\nP3\n49,270\n\n\nB\nP3\n49,664\n\n\nC\nP3\n48,507\n\n\n\nThis dataset is said to have a long format because its first column (Store) has values repeating in multiple rows. We would like to see how the sales values for products P1, P2, and P3 change across stores A, B, and C, and a clustered bar graph is ideal for visualizing these changes. We use pandas to recreate the dataset in Python. Here is the code:\n\n# Create dataset\ndf = pd.DataFrame({\"Store\": [\"A\", \"B\", \"C\", \"A\", \"B\", \"C\", \"A\", \"B\", \"C\"], \"Product\": [\"P1\", \"P1\", \"P1\", \"P2\", \"P2\", \"P2\", \"P3\", \"P3\", \"P3\"], \"Sales\": [42500, 48502, 39500, 43600, 32303, 29500, 49270, 49664, 48507]})\nprint(df)\n\n  Store Product  Sales\n0     A      P1  42500\n1     B      P1  48502\n2     C      P1  39500\n3     A      P2  43600\n4     B      P2  32303\n5     C      P2  29500\n6     A      P3  49270\n7     B      P3  49664\n8     C      P3  48507"
  },
  {
    "objectID": "posts/clustered-barplots-python.html#creating-the-graph",
    "href": "posts/clustered-barplots-python.html#creating-the-graph",
    "title": "Creating clustered bar graphs with Python",
    "section": "Creating the graph",
    "text": "Creating the graph\n\nMethod 1: Using Plotly\nYou can use either the plotly.express or the plotly.graph_objects module to plot graphs in Plotly. The plotly.graph_objects module is best for building complex graphs requiring significant customization.\nTo build the clustered bar graph in plotly.express, use the following code:\n\n# plotly.express clustered barchart\nfig = px.bar(df, x=\"Store\", y=\"Sales\",\n             color=\"Product\", barmode=\"group\", title=\"Product sales in $\",\n             color_discrete_sequence=px.colors.qualitative.Set2,\n             width=500, height=400,\n             text_auto=True, template=\"plotly_white\")\nfig.update_yaxes(visible=False)\nfig.show()\n\n\n\n                                                \n\n\nFigure 1: Clustered bar graph created using plotly.express.\n\n\n\n\nYou can learn more about the parameters of px.bar() by using the command help(px.bar).\nIn the code above, we use the update_yaxes() figure method to suppress the graph’s y-axis and horizontal grid lines, removing graph clutter. The result is displayed in Figure 1.\nNext, we create another clustered bar graph using plotly.graph_objects. However, to use this module, you must reorganize the dataset into a wide format, ensuring that each row in the new dataset represents a unique store.\nAs the data is small, you can recreate it from scratch using the following code:\n\ndf_wide = pd.DataFrame({\"Store\": [\"A\", \"B\", \"C\"], \"P1\": [42500, 48502, 39500], \n              \"P2\": [43600, 32303, 29500], \"P3\": [49270, 49664, 48507]})\nprint(df_wide)\n\n  Store     P1     P2     P3\n0     A  42500  43600  49270\n1     B  48502  32303  49664\n2     C  39500  29500  48507\n\n\nIf the data were large, it’d be easier to reorganize it using the pivot() method in pandas. This method has the following three crucial parameters:\n\ncolumns: columns of the new DataFrame.\nindex: column used to create the new DataFrame’s index.\nvalues: columns used to populate the new DataFrame.\n\nUse the command help(df.pivot) to learn more about pivot().\nFor this case, you can use pivot() to reorganize the data as follows:\n\ndf_wideP = df.pivot(columns=[\"Product\"], index=\"Store\", values=\"Sales\")\nprint(df_wideP)\n\nProduct     P1     P2     P3\nStore                       \nA        42500  43600  49270\nB        48502  32303  49664\nC        39500  29500  48507\n\n\nHaving reorganized the data, we can now use plotly.graph_objects to visualize it. The result is the graph in Figure 2.\n\ndata=[go.Bar(name=\"P1\", x=df_wide[\"Store\"], y=df_wide[\"P1\"], text=df_wide[\"P1\"], marker_color=\"rgb(102, 194, 165)\"),\n    go.Bar(name=\"P2\", x=df_wide[\"Store\"], y=df_wide[\"P2\"], text=df_wide[\"P2\"], marker_color=\"rgb(252, 141, 98)\"),\n    go.Bar(name=\"P3\", x=df_wide[\"Store\"], y=df_wide[\"P3\"], text=df_wide[\"P3\"], marker_color=\"rgb(141, 160, 203)\")]\nlayout = go.Layout(\n    title={\"text\": \"Product sales in $\"},\n    xaxis={\"title\": \"Store\"},\n    legend={\"title\": \"Product\"},\n    width=500, height=400, template=\"plotly_white\",\n    barmode=\"group\"\n)\nfig_go = go.Figure(data=data, layout=layout)\nfig_go.update_yaxes(visible=False)\nfig_go.show()\n\n\n\n                                                \n\n\nFigure 2: Clustered bar graph created using plotly.graph_objects.\n\n\n\n\n\n\nMethod 2: Using Matplotlib\nAs we already have a wide pandas DataFrame, i.e., df_wide, we can call pandas’ plot() method on this DataFrame to plot the required graph. This method leverages Matplotlib’s plotting capabilities, resulting in the graph in Figure 3. Here’s the code:\n\n# Clustered bar graph using pandas' plot() method\ncolors = [\"#816e94\", \"#22a4bf\", \"#8cc781\"]\ndf_wide.plot(x=\"Store\", \n        kind=\"bar\", width=0.9,\n        stacked=False, figsize=(5, 4), title=\"Product sales in $\",\n        xlabel=\"Store\", ylabel=\"Sales\", color=colors)\nplt.legend(loc=\"lower right\", title=\"Product\");\n\n\n\n\n\n\n\nFigure 3: Clustered bar graph created using pandas’ plot() method.\n\n\n\n\n\nAlternatively, you can use matplotlib’s bar() method, as shown below.\n\n# Clustered bar graph using matplotlib's bar() method\nbarWidth = 0.25\ncolors = [\"#816e94\", \"#22a4bf\", \"#8cc781\"]\n\n# Sales values for products p1, p2, and p3\np1 = df_wide[\"P1\"] \np2 = df_wide[\"P2\"]\np3 = df_wide[\"P3\"]\n\n# Positions of bar groups along the x-axis\npos1 = np.arange(len(p1))  \npos2 = pos1 + barWidth\npos3 = pos2 + barWidth  \n\n# Plot graph\nfig, ax = plt.subplots(figsize=(5, 4))\nax.bar(pos1, p1, color=colors[0], width=barWidth, label=\"P1\")\nax.bar(pos2, p2, color=colors[1], width=barWidth, label=\"P2\")\nax.bar(pos3, p3, color=colors[2], width=barWidth, label=\"P3\")\nax.set_xlabel(\"Store\")\nax.set_xticks(pos1 + barWidth)  # Set x-ticks to be in the middle of the bar groups\nax.set_xticklabels([\"A\", \"B\", \"C\"])  # store group labels\nax.legend(loc=\"lower right\", title=\"Product\")\nplt.title(\"Product sales in $\")\nplt.ylabel(\"Sales\")\nplt.show()\n\n\n\n\n\n\n\nFigure 4: Clustered bar graph created using matplotlib’s bar() method.\n\n\n\n\n\nAs you can see, this method builds every bar from the ground up. To use it effectively, you must specify the positions of the bar groups and ticks along the x-axis and the width of the bars. In this case, the first, second, and third groups start at 0, 1, and 2, respectively. Each group contains three bars (for P1, P2, and P3), so the total width of these three bars must be less than 1 unit, with each bar having a maximum width of 0.33 units. If the width of a bar exceeds 0.33 units, the bar groups will touch each other or overlap. We set the bar width at 0.25 units, resulting in the graph shown in Figure 4.\n\n\nMethod 3: Using Seaborn\nThe Seaborn library is built on top of Matplotlib and provides many tools to enhance Matplotlib’s plotting capabilities.\nWe use Seaborn’s barplot() method to plot the clustered bar graph. This method supports both wide- and long-format data. However, it’s easier to use with long-format data where critical variables are not spread over multiple columns. Here’s the code:\n\n# Clustered bar graph using Seaborn's barplot() method\ncustom_palette = [\"#a6cee3\", \"#1f78b4\", \"#b2df8a\"]\nfig, ax = plt.subplots(figsize=(5, 4)) # Set plot dimensions.\nsns.set_style(\"white\")\nsns.barplot(x=\"Store\", y=\"Sales\", hue=\"Product\", data=df, palette=custom_palette).set(title=\"Product sales in $\")\nplt.legend(loc=\"lower right\")\nplt.show()\n\n\n\n\n\n\n\nFigure 5: Clustered bar graph created using Seaborn’s barplot() method.\n\n\n\n\n\nMany methods used in the code above, like plt.show() and plt.subplots(), are familiar, as we’ve already used them when plotting with Matplotlib.\nThe code outputs the graph in Figure 5."
  },
  {
    "objectID": "posts/clustered-barplots-python.html#conclusion",
    "href": "posts/clustered-barplots-python.html#conclusion",
    "title": "Creating clustered bar graphs with Python",
    "section": "Conclusion",
    "text": "Conclusion\nBeing Python’s first visualization library, Matplotlib has become a favorite of many Python users. Several later libraries, like pandas and Seaborn, are built on top of it because it is highly customizable and flexible. However you may have to write a lot of code with Matplotlib to create appealing graphs. Generally, of all libraries discussed in this article, the plotly.express module is the easiest to use for creating clustered bar graphs and provides the most appealing results."
  },
  {
    "objectID": "posts/logistic-regression-adult-data.html",
    "href": "posts/logistic-regression-adult-data.html",
    "title": "Using logistic regression to predict income levels",
    "section": "",
    "text": "The Adult dataset (Becker and Kohavi 1996), accessible from the UCI Machine Learning Repository, is a sample from the 1994 US census database comprising 15 variables. A description of each variable is given below.\n\n\nVariable\nDescription\n\n\n\nage\nAge\n\n\nworkclass\nEmployment status\n\n\nfnlwgt\nFinal weight, i.e., the number of people in a given row\n\n\neducation\nHighest level of education\n\n\neducation-num\nHighest level of education in numerical form\n\n\nmarital_status\nMarital status\n\n\noccupation\nOccupation\n\n\nrelationship\nFamily relationship\n\n\nrace\nRace\n\n\nsex\nSex/gender\n\n\ncapital_gain\nCapital gain for an individual\n\n\ncapital_loss\nCapital loss for an individual\n\n\nhours_per_week\nWorking hours per week\n\n\nnative_country\nCountry of birth\n\n\nincome\nWhether an individual makes more than $50,000 annually\n\n\n\nObjective: Use logistic regression to build a model that predicts a person’s income group, categorized as &lt;=50K or &gt;50K.\nAs we have already downloaded the data, we load it in R using the code below. The dataset consists of two parts: the training set, adult.data, and the test set, adult.test. We start by loading the training set, which is vital for exploration exercises.\n\n# Read train data\ntrain_data &lt;- read.csv(\"C:/Users/User/Desktop/Logistic_Regression_ADULT/Data/adult.data\", header = FALSE, sep = \",\", strip.white = TRUE, stringsAsFactors = TRUE)\n# Name columns\nnames(train_data) &lt;- c(\"age\", \"workclass\", \"fnlwgt\", \"education\", \"education_num\", \"marital_status\", \"occupation\", \"relationship\", \"race\", \"sex\", \"capital_gain\", \"capital_loss\", \"hours_per_week\", \"native_country\", \"income\")"
  },
  {
    "objectID": "posts/logistic-regression-adult-data.html#dataset-information",
    "href": "posts/logistic-regression-adult-data.html#dataset-information",
    "title": "Using logistic regression to predict income levels",
    "section": "",
    "text": "The Adult dataset (Becker and Kohavi 1996), accessible from the UCI Machine Learning Repository, is a sample from the 1994 US census database comprising 15 variables. A description of each variable is given below.\n\n\nVariable\nDescription\n\n\n\nage\nAge\n\n\nworkclass\nEmployment status\n\n\nfnlwgt\nFinal weight, i.e., the number of people in a given row\n\n\neducation\nHighest level of education\n\n\neducation-num\nHighest level of education in numerical form\n\n\nmarital_status\nMarital status\n\n\noccupation\nOccupation\n\n\nrelationship\nFamily relationship\n\n\nrace\nRace\n\n\nsex\nSex/gender\n\n\ncapital_gain\nCapital gain for an individual\n\n\ncapital_loss\nCapital loss for an individual\n\n\nhours_per_week\nWorking hours per week\n\n\nnative_country\nCountry of birth\n\n\nincome\nWhether an individual makes more than $50,000 annually\n\n\n\nObjective: Use logistic regression to build a model that predicts a person’s income group, categorized as &lt;=50K or &gt;50K.\nAs we have already downloaded the data, we load it in R using the code below. The dataset consists of two parts: the training set, adult.data, and the test set, adult.test. We start by loading the training set, which is vital for exploration exercises.\n\n# Read train data\ntrain_data &lt;- read.csv(\"C:/Users/User/Desktop/Logistic_Regression_ADULT/Data/adult.data\", header = FALSE, sep = \",\", strip.white = TRUE, stringsAsFactors = TRUE)\n# Name columns\nnames(train_data) &lt;- c(\"age\", \"workclass\", \"fnlwgt\", \"education\", \"education_num\", \"marital_status\", \"occupation\", \"relationship\", \"race\", \"sex\", \"capital_gain\", \"capital_loss\", \"hours_per_week\", \"native_country\", \"income\")"
  },
  {
    "objectID": "posts/logistic-regression-adult-data.html#exploratory-data-analysis",
    "href": "posts/logistic-regression-adult-data.html#exploratory-data-analysis",
    "title": "Using logistic regression to predict income levels",
    "section": "Exploratory Data Analysis",
    "text": "Exploratory Data Analysis\nLogistic regression makes the following assumptions about the data:\n\nThe target variable is binary.\nAbsence of multicollinearity among the predictor variables.\nThere are no outliers in the data.\nA linear relationship exists between continuous predictors and the logit of the outcome.\n\nWe perform exploratory data analysis (EDA) before building the model to check whether the data meets these assumptions. EDA also helps with feature selection and reveals possible relationships between variables.\nTask 1: Load required packages.\n\nlibrary(ggplot2)\nlibrary(cowplot)\nlibrary(caret)\nlibrary(dplyr)\n\nTask 2: Determine the internal structure of the dataset.\nWe use the str() function to gain an overview of the dataset. This function provides information on dataset properties like dimension and data types.\n\nstr(train_data)\n\n'data.frame':   32561 obs. of  15 variables:\n $ age           : int  39 50 38 53 28 37 49 52 31 42 ...\n $ workclass     : Factor w/ 9 levels \"?\",\"Federal-gov\",..: 8 7 5 5 5 5 5 7 5 5 ...\n $ fnlwgt        : int  77516 83311 215646 234721 338409 284582 160187 209642 45781 159449 ...\n $ education     : Factor w/ 16 levels \"10th\",\"11th\",..: 10 10 12 2 10 13 7 12 13 10 ...\n $ education_num : int  13 13 9 7 13 14 5 9 14 13 ...\n $ marital_status: Factor w/ 7 levels \"Divorced\",\"Married-AF-spouse\",..: 5 3 1 3 3 3 4 3 5 3 ...\n $ occupation    : Factor w/ 15 levels \"?\",\"Adm-clerical\",..: 2 5 7 7 11 5 9 5 11 5 ...\n $ relationship  : Factor w/ 6 levels \"Husband\",\"Not-in-family\",..: 2 1 2 1 6 6 2 1 2 1 ...\n $ race          : Factor w/ 5 levels \"Amer-Indian-Eskimo\",..: 5 5 5 3 3 5 3 5 5 5 ...\n $ sex           : Factor w/ 2 levels \"Female\",\"Male\": 2 2 2 2 1 1 1 2 1 2 ...\n $ capital_gain  : int  2174 0 0 0 0 0 0 0 14084 5178 ...\n $ capital_loss  : int  0 0 0 0 0 0 0 0 0 0 ...\n $ hours_per_week: int  40 13 40 40 40 40 16 45 50 40 ...\n $ native_country: Factor w/ 42 levels \"?\",\"Cambodia\",..: 40 40 40 40 6 40 24 40 40 40 ...\n $ income        : Factor w/ 2 levels \"&lt;=50K\",\"&gt;50K\": 1 1 1 1 1 1 1 2 2 2 ...\n\n\nThe output from str() shows that the training dataset has 32,561 observations and 15 variables. Our target variable is income, which has two categories &lt;=50K and &gt;50K. Therefore, we can use logistic regression to build a binary classification model.\nWe also notice that the factor variables workclass, occupation, and native_country include levels denoted by a “?”. Upon examining the Variables Table provided at the UCI Machine Learning Repository within the Dataset Information section, we find that these columns have missing values. We conclude that the “?” symbol represents missing data.\nTask 3: Handle missing data\nThe presence of missing data in a regression model can significantly impair the model’s accuracy in predicting responses. In this task, we examine the proportion of missing values in the workclass, occupation, and native_country columns of the training data.\n\n# Percentage of missing values in workclass\nnrow(train_data[train_data$workclass == '?', ]) * 100/nrow(train_data)\n\n[1] 5.638647\n\n# Percentage of missing values in occupation\nnrow(train_data[train_data$occupation == '?', ]) * 100/nrow(train_data)\n\n[1] 5.660146\n\n# Percentage of missing values in native_country\nnrow(train_data[train_data$native_country == '?', ]) * 100/nrow(train_data)\n\n[1] 1.790486\n\n\nWe find that the highest percentage of missing values occurs in the workclass and occupation variables, with about 5% of the training data missing in each case. As we’d like to retain these missing values, we recode them as “Missing”, creating an additional category for each variable.\n\ntrain_data$workclass &lt;- recode(train_data$workclass, \"?\" = \"Missing\")\ntrain_data$occupation &lt;- recode(train_data$occupation, \"?\" = \"Missing\")\ntrain_data$native_country &lt;- recode(train_data$native_country, \"?\" = \"Missing\")\n\nTask 4: Discard zero-variance and near-zero variance independent variables.\nA zero-variance variable has only one unique value, while a near-zero-variance variable has very few unique values. These variables are also termed low cardinality variables, where cardinality refers to the number of unique values a variable has. Because they contain little information, they are unimportant to the analysis (Boehmke and Greenwell 2019). We use the nearZeroVar() function from the caret package to identify these variables in the training data.\n\nX = nearZeroVar(train_data, saveMetrics = TRUE)\nX\n\n                freqRatio percentUnique zeroVar   nzv\nage              1.011261   0.224194589   FALSE FALSE\nworkclass        8.931917   0.027640429   FALSE FALSE\nfnlwgt           1.000000  66.484444581   FALSE FALSE\neducation        1.440269   0.049138540   FALSE FALSE\neducation_num    1.440269   0.049138540   FALSE FALSE\nmarital_status   1.401853   0.021498111   FALSE FALSE\noccupation       1.010002   0.046067381   FALSE FALSE\nrelationship     1.588561   0.018426952   FALSE FALSE\nrace             8.903969   0.015355794   FALSE FALSE\nsex              2.023025   0.006142317   FALSE FALSE\ncapital_gain    86.020173   0.365467891   FALSE  TRUE\ncapital_loss   153.673267   0.282546605   FALSE  TRUE\nhours_per_week   5.398013   0.288688922   FALSE FALSE\nnative_country  45.365474   0.128988667   FALSE  TRUE\nincome           3.152659   0.006142317   FALSE FALSE\n\n\nFrom the above output, the training data has no zero-variance variables but contains several near-zero-variance variables, namely, capital_gain, capital_loss, and native_country. We use histograms to explore capital_gain and capital_loss further.\n\n# Remove scientific notation\noptions(scipen = 999)\n\n# Histogram of Capital Gain\ncg_plot &lt;- ggplot(data = train_data, aes(x = capital_gain)) + geom_histogram(bins = 20) + labs(x = \"Capital Gain\", y = \"Count\") + theme(axis.title = element_text(family = \"Arial\"), plot.title = element_text(family = \"Arial\")) + theme_minimal_grid(12)\n\n# Histogram of Capital Loss\ncl_plot &lt;- ggplot(data = train_data, aes(x = capital_loss)) + geom_histogram(bins = 20) + labs(x = \"Capital Loss\", y = \"Count\") + theme(axis.title = element_text(family = \"Arial\"), plot.title = element_text(family = \"Arial\")) + theme_minimal_grid(12)\n\n# Use plot_grid() from cowplot to arrange plots on grid\nplot_grid(cg_plot, cl_plot, labels = c('A', 'B'), label_size = 12)\n\n\n\n\n\n\nFigure 1: Distribution of A) capital_gain and B) capital_loss among the sample population.\n\n\n\n\nFigure 1 reveals that the distributions of capital_gain and capital_loss are strongly skewed to the right, with most observations being zero. Therefore, we discard these two variables.\nIn addition, the frequency distribution of native_country shown below indicates that almost 90% (29170/32561) of all entries fall within the United-States category. Because of this heavy skewness, we exclude this variable from our analysis.\n\n# Frequencies of levels in native_country\nsummary(train_data$native_country)\n\n                   Missing                   Cambodia \n                       583                         19 \n                    Canada                      China \n                       121                         75 \n                  Columbia                       Cuba \n                        59                         95 \n        Dominican-Republic                    Ecuador \n                        70                         28 \n               El-Salvador                    England \n                       106                         90 \n                    France                    Germany \n                        29                        137 \n                    Greece                  Guatemala \n                        29                         64 \n                     Haiti         Holand-Netherlands \n                        44                          1 \n                  Honduras                       Hong \n                        13                         20 \n                   Hungary                      India \n                        13                        100 \n                      Iran                    Ireland \n                        43                         24 \n                     Italy                    Jamaica \n                        73                         81 \n                     Japan                       Laos \n                        62                         18 \n                    Mexico                  Nicaragua \n                       643                         34 \nOutlying-US(Guam-USVI-etc)                       Peru \n                        14                         31 \n               Philippines                     Poland \n                       198                         60 \n                  Portugal                Puerto-Rico \n                        37                        114 \n                  Scotland                      South \n                        12                         80 \n                    Taiwan                   Thailand \n                        51                         18 \n           Trinadad&Tobago              United-States \n                        19                      29170 \n                   Vietnam                 Yugoslavia \n                        67                         16 \n\n# Drop non-informative columns\ntrain_data$capital_gain &lt;- NULL\ntrain_data$capital_loss &lt;- NULL\ntrain_data$native_country &lt;- NULL\n\nTask 5: Discard closely related independent variables.\nTwo variables are considered collinear when they are so strongly correlated that it is difficult to determine their individual effects on the target variable. When this type of relationship occurs between more than two variables, we refer to it as multicollinearity. In the training dataset, education and education_num are collinear, as education_num is simply a numeric representation of education. The two variables contain the same information, so we discard one, i.e., education_num.\n\n# Drop one of a pair of collinear variables\ntrain_data$education_num &lt;- NULL\n\nTask 6: Discard variables that aren’t important to the analysis.\nHere, we discard fnlwgt and relationship.\n\n# Drop irrelevant variables\ntrain_data$fnlwgt &lt;- NULL\ntrain_data$relationship &lt;- NULL\n\nTask 7: Lump levels of independent categorical features with few observations.\nThe independent categorical variables in the dataset are workclass, education, marital_status, occupation, race, and sex. We explore each of these variables to determine instances where level lumping can be applied to reduce the noise in the data.\nLump workclass levels\nWe use a barplot to compare the number of people belonging to each workclass level.\n\n# Barplot of workclass\nwc_plot &lt;- ggplot(data = train_data, aes(x = workclass)) + geom_bar(fill=\"steelblue\") + labs(x = \"Work Class\", y = \"Count\") + theme(axis.title = element_text(family = \"Arial\")) + theme_cowplot(12) + coord_flip()\n\nwc_plot\n\n\n\n\n\n\nFigure 2: Distribution of individuals across workclass levels.\n\n\n\n\nFigure 2 shows that the two levels, Without-pay and Never-worked have very few observations. So, we lump them together with the Missing in a level called “Other/Missing”. We also lump State-gov, Local-gov, and Federal-gov into a new level called “Gov,” as they all pertain to government-related work. Self-emp-not-inc and Self-emp-inc are consolidated into a new level called “Self-emp,” as both are related to self-employment.\n\n# Change names of workclass levels\ntrain_data$workclass &lt;- recode(train_data$workclass, \"Missing\" = \"Other/Missing\", \"Federal-gov\" = \"Gov\", \"Local-gov\" = \"Gov\", \"Never-worked\" = \"Other/Missing\", \"Self-emp-inc\" = \"Self-emp\", \"Self-emp-not-inc\" = \"Self-emp\", \"State-gov\" = \"Gov\", \"Without-pay\" = \"Other/Missing\")\n\n# Check workclass level frequencies\ntable(train_data$workclass)\n\n\nOther/Missing           Gov       Private      Self-emp \n         1857          4351         22696          3657 \n\n\nWe want the first (base) level of workclass to be a meaningful quantity that can later help with interpreting regression results. So, we use the relevel() function to reorder these levels, removing “Other/Missing” as the base level.\n\ntrain_data$workclass &lt;- relevel(train_data$workclass, ref = \"Gov\")\n\nLump education levels\nThe barplot in Figure 3 explores the distribution of people by education. Here is the code used to create this plot:\n\n# Barplot of education\ned_plot &lt;- ggplot(data = train_data, aes(x = education)) + geom_bar(fill=\"steelblue\") + labs(x = \"Education\", y = \"Count\") + theme(axis.title = element_text(family = \"Arial\")) + theme_cowplot(12) + coord_flip()\n\ned_plot\n\n\n\n\n\n\nFigure 3: Distribution of individuals across education levels.\n\n\n\n\nWe see that all levels from Preschool to 12th grade have somewhat low frequencies. So, we lump these levels into a new category called Preschool-12th. We also merge Assoc-voc and Assoc-acdm into a single level called Associate, as shown in the code below.\n\n# Recode education levels\ntrain_data$education &lt;- recode(train_data$education, \"10th\" = \"Preschool-12th\", \"11th\" = \"Preschool-12th\", \"12th\" = \"Preschool-12th\", \"1st-4th\" = \"Preschool-12th\", \"5th-6th\" = \"Preschool-12th\", \"7th-8th\" = \"Preschool-12th\", \"9th\" = \"Preschool-12th\", \"Assoc-acdm\" = \"Associate\", \"Assoc-voc\" = \"Associate\", \"Preschool\" = \"Preschool-12th\")\n\ntable(train_data$education)\n\n\nPreschool-12th      Associate      Bachelors      Doctorate        HS-grad \n          4253           2449           5355            413          10501 \n       Masters    Prof-school   Some-college \n          1723            576           7291 \n\n\nLump marital_status levels\nThe barplot in Figure 4 explores the distribution of people by marital_status.\n\n# Barplot of marital_status\nms_plot &lt;- ggplot(data = train_data, aes(x = marital_status)) + geom_bar(fill=\"steelblue\") + labs(x = \"Marital Status\", y = \"Count\") + theme(axis.title = element_text(family = \"Arial\")) + theme_cowplot(12) + coord_flip()\n\nms_plot\n\n\n\n\n\n\nFigure 4: Distribution of individuals across marital_status levels.\n\n\n\n\nWe lump Married-spouse-absent, Married-civ-spouse, and Married-AF-spouse into a level called “Married”, as they all denote married people. By doing so, we reduce the number of levels in this variable to 5.\n\n# Recode marital_status levels\ntrain_data$marital_status &lt;- recode(train_data$marital_status, \"Married-AF-spouse\" = \"Married\", \"Married-civ-spouse\" = \"Married\", \"Married-spouse-absent\" = \"Married\")\n\ntable(train_data$marital_status)\n\n\n     Divorced       Married Never-married     Separated       Widowed \n         4443         15417         10683          1025           993 \n\n\nLump occupation levels\nThe barplot in Figure 5 illustrates the counts of people in each level of occupation. Here is the code used to create this plot.\n\noc_plot &lt;- ggplot(data = train_data, aes(x = occupation)) + geom_bar(fill=\"steelblue\") + labs(x = \"Occupation\", y = \"Count\") + theme(axis.title = element_text(family = \"Arial\")) + theme_cowplot(12) + coord_flip()\n\noc_plot\n\n\n\n\n\n\nFigure 5: Distribution of individuals across occupation levels.\n\n\n\n\nWe create the following new levels for occupation:\n\nService-sales-workers: Protective-serv, Priv-house-serv, Other-service, Sales\nManagers: Exec-managerial\nProfessionals: Prof-specialty\nTechnical: Tech-support\nClerical: Adm-clerical\nLabor-trades: Machine-op-inspct, Handlers-cleaners, Farming-fishing, Craft-repair, Transport-moving\nOthers: Armed-Forces, Missing\n\n\n# Recode occupation levels\ntrain_data$occupation &lt;- recode(train_data$occupation, \"Protective-serv\" = \"Service-sales-workers\", \"Sales\" = \"Service-sales-workers\", \"Priv-house-serv\" = \"Service-sales-workers\", \"Other-service\" = \"Service-sales-workers\", \"Exec-managerial\" = \"Managers\", \"Prof-specialty\" = \"Professionals\", \"Tech-support\" = \"Technical\", \"Adm-clerical\" = \"Clerical\", \"Machine-op-inspct\" = \"Labor-trades\", \"Handlers-cleaners\" = \"Labor-trades\", \"Farming-fishing\" = \"Labor-trades\", \"Craft-repair\" = \"Labor-trades\", \"Transport-moving\" = \"Labor-trades\", \"Armed-Forces\" = \"Other/Missing\", \"Missing\" = \"Other/Missing\")\n\ntable(train_data$occupation)\n\n\n        Other/Missing              Clerical          Labor-trades \n                 1852                  3770                 10062 \n             Managers Service-sales-workers         Professionals \n                 4066                  7743                  4140 \n            Technical \n                  928 \n\n\nWe also reorder the levels of occupation so that the first level is Clerical, a more meaningful quantity than Other/Missing.\n\ntrain_data$occupation &lt;- relevel(train_data$occupation, ref = \"Clerical\")\n\nLump race levels\nThe barplot in Figure 6 shows the counts of people by race. Here is the code used to create it:\n\nr_plot &lt;- ggplot(data = train_data, aes(x = race)) + geom_bar(fill=\"steelblue\") + labs(x = \"Race\", y = \"Count\") + theme(axis.title = element_text(family = \"Arial\")) + theme_cowplot(12) + coord_flip()\n\nr_plot\n\n\n\n\n\n\nFigure 6: Distribution of individuals by race.\n\n\n\n\nWe see that more than 20000 people are white, and the next most populous race is black, with less than 5000 people. To simplify our analysis, we regroup race into two levels, White and Non-white.\n\n# Recode race levels\ntrain_data$race &lt;- recode(train_data$race, \"Amer-Indian-Eskimo\" = \"Non-white\", \"Asian-Pac-Islander\" = \"Non-white\", \"Black\" = \"Non-white\", \"Other\" = \"Non-white\")\n\ntable(train_data$race)\n\n\nNon-white     White \n     4745     27816 \n\n\nLump sex levels\nFigure 7 shows the number of people in each level of sex. Here is the code used to create it:\n\ns_plot &lt;- ggplot(data = train_data, aes(x = sex)) + geom_bar(fill=\"steelblue\") + labs(x = \"Sex\", y = \"Count\") + theme(axis.title = element_text(family = \"Arial\")) + theme_cowplot(12) + coord_flip()\n\ns_plot\n\n\n\n\n\n\nFigure 7: Distribution of individuals by sex.\n\n\n\n\nWe see that most people in the data are male. As sex only has two levels, both of which are important to the analysis, we don’t do any lumping here.\nTask 8: Study the relationship between income and each of the independent variables.\nBefore we build our model, we want to see if there are specific differences between people with incomes &lt;=50K and those with incomes &gt;50K. So, we examine the relationship between income and each independent variable.\nIncome vs. Age\nWe use a histogram to determine the relationship between age and income.\n\nage_plot &lt;- ggplot(train_data, aes(x = age, fill = income)) +\n    geom_histogram(position = \"identity\", alpha = 0.4, bins = 30) + labs(x = \"Age\", y = \"Count\", fill = \"Income\") + theme(axis.title = element_text(family = \"Arial\")) + theme_cowplot(12)\n\nage_plot\n\n\n\n\n\n\nFigure 8: Distribution of income by age.\n\n\n\n\nFigure 8 shows that more people have incomes of &lt;=50K than of &gt;50K. The minimum age for people with incomes &lt;=50K is slightly less than that for people of the other income group. Also, the distribution of ages in the &lt;=50K group is heavily skewed to the right, suggesting that the mean age of this group is higher than the median age.\nIncome vs. workclass\nHere is the code used to create the proportional stacked barplot in Figure 9, showing the relationship between workclass and income:\n\nwc_plot2 &lt;- ggplot(data = train_data, aes(x = workclass, fill = income)) + geom_bar(position = \"fill\") + scale_fill_manual(values = c(\"#f0b27a\", \"#85c1e9\")) + labs(x = \"Work Class\", y = \"Proportion\", fill = \"Income\") + theme(axis.title = element_text(family = \"Arial\")) + theme_cowplot(12)\n\nwc_plot2\n\n\n\n\n\n\nFigure 9: Distribution of income across workclass levels.\n\n\n\n\nFrom the barplot, we see that the workclass with the highest proportion of &gt;50K income earners is Self-emp, while that with the smallest proportion of this group of earners is Other/Missing.\nIncome vs. Education\nHere is the code for creating the barplot in Figure 10:\n\ned_plot2 &lt;- ggplot(data = train_data, aes(x = education, fill = income)) + geom_bar(position = \"fill\") + scale_fill_manual(values = c(\"#f0b27a\", \"#85c1e9\")) + labs(x = \"Education\", y = \"Proportion\", fill = \"Income\") + theme(axis.title = element_text(family = \"Arial\")) + theme_cowplot(12) + coord_flip()\n\ned_plot2\n\n\n\n\n\n\nFigure 10: Distribution of income across education levels.\n\n\n\n\nThe barplot reveals that the Prof-school and Doctorate levels of education have the highest proportions of people with an annual income &gt;50K. Also, the proportion of &gt;50K income earners appears to increase with an increase in the level of education.\nIncome vs. Marital Status\nWe use the code below to create the barplot in Figure 11:\n\nms_plot2 &lt;- ggplot(data = train_data, aes(x = marital_status, fill = income)) + geom_bar(position = \"fill\") + scale_fill_manual(values = c(\"#f0b27a\", \"#85c1e9\")) + labs(x = \"Marital Status\", y = \"Proportion\", fill = \"Income\") + theme(axis.title = element_text(family = \"Arial\")) + theme_cowplot(12) + coord_flip()\n\nms_plot2\n\n\n\n\n\n\nFigure 11: Distribution of income across marital_status levels.\n\n\n\n\nThe barplot shows that the highest proportion of &gt;50K income earners are found in the Married group, followed by the Divorced, Widowed, Separated, and Never-married groups, in that order.\nIncome vs. Occupation\nWe create the barplot in Figure 12 using the following code:\n\noc_plot2 &lt;- ggplot(data = train_data, aes(x = occupation, fill = income)) + geom_bar(position = \"fill\") + scale_fill_manual(values = c(\"#f0b27a\", \"#85c1e9\")) + labs(x = \"Occupation\", y = \"Proportion\", fill = \"Income\") + theme(axis.title = element_text(family = \"Arial\")) + theme_cowplot(12) + coord_flip()\n\noc_plot2\n\n\n\n\n\n\nFigure 12: Distribution of income across occupation levels.\n\n\n\n\nFrom the barplot, we see that the highest proportion of &gt;50K income earners are found in the Managers group, followed by the Professionals and Technical groups. The Other/Missing group has the lowest proportion of &gt;50K income earners.\nIncome vs. Race\nThe code below creates the barplot in Figure 13, showing the distribution of income across race levels.\n\nr_plot2 &lt;- ggplot(data = train_data, aes(x = race, fill = income)) + geom_bar(position = \"fill\") + scale_fill_manual(values = c(\"#f0b27a\", \"#85c1e9\")) + labs(x = \"Race\", y = \"Proportion\", fill = \"Income\") + theme(axis.title = element_text(family = \"Arial\")) + theme_cowplot(12)\n\nr_plot2\n\n\n\n\n\n\nFigure 13: Distribution of income by race.\n\n\n\n\nThe barplot shows that the White group has a larger proportion of &gt;50K income earners than the Non-white group.\nIncome vs. Hours/Week\nWe use the code below to plot the histogram in Figure 14, showing the relationship between income and hours_per_week.\n\nhpw_plot &lt;- ggplot(train_data, aes(x = hours_per_week, fill = income)) +\n    geom_histogram(position = \"identity\", alpha = 0.4, bins = 30) + labs(x = \"Hours/Week\", y = \"Count\", fill = \"Income\") + theme(axis.title = element_text(family = \"Arial\")) + theme_cowplot(12)\n\nhpw_plot\n\n\n\n\n\n\nFigure 14: Distribution of income by hours_per_week.\n\n\n\n\nFrom the histogram, we see that most people in the training data work about 40 hours/week. A large proportion of those who work less than 40 hours/week are &lt;=50K income earners. However, looking at the distribution of the &gt;50K income earners, the bars to the right of the 40 hours/week are longer than those to the left, meaning that many people in this income group work more than 40 hours/week."
  },
  {
    "objectID": "posts/logistic-regression-adult-data.html#build-model",
    "href": "posts/logistic-regression-adult-data.html#build-model",
    "title": "Using logistic regression to predict income levels",
    "section": "Build Model",
    "text": "Build Model\nTask 9: Build a binary logistic regression model.\nWe use the glm() function to build a model that predicts income group based on the variables age, workclass, education, marital_status, occupation, race, sex, and hours_per_week.\n\noptions(scipen=0)\nmodel1 &lt;- glm(income ~ ., family = \"binomial\", data = train_data)\nsummary(model1)\n\n\nCall:\nglm(formula = income ~ ., family = \"binomial\", data = train_data)\n\nCoefficients:\n                                 Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                     -6.914094   0.154482 -44.757  &lt; 2e-16 ***\nage                              0.028700   0.001488  19.291  &lt; 2e-16 ***\nworkclassOther/Missing          -1.586627   0.745594  -2.128  0.03334 *  \nworkclassPrivate                 0.051427   0.047502   1.083  0.27898    \nworkclassSelf-emp               -0.202010   0.061587  -3.280  0.00104 ** \neducationAssociate               1.706695   0.089867  18.991  &lt; 2e-16 ***\neducationBachelors               2.315040   0.081844  28.286  &lt; 2e-16 ***\neducationDoctorate               3.310179   0.157257  21.049  &lt; 2e-16 ***\neducationHS-grad                 1.102238   0.076070  14.490  &lt; 2e-16 ***\neducationMasters                 2.677012   0.097854  27.357  &lt; 2e-16 ***\neducationProf-school             3.338942   0.139353  23.960  &lt; 2e-16 ***\neducationSome-college            1.481604   0.079442  18.650  &lt; 2e-16 ***\nmarital_statusMarried            1.990770   0.060212  33.063  &lt; 2e-16 ***\nmarital_statusNever-married     -0.510628   0.074723  -6.834 8.28e-12 ***\nmarital_statusSeparated         -0.143951   0.145438  -0.990  0.32228    \nmarital_statusWidowed           -0.018087   0.134534  -0.134  0.89305    \noccupationOther/Missing          0.874156   0.746557   1.171  0.24163    \noccupationLabor-trades          -0.323266   0.067238  -4.808 1.53e-06 ***\noccupationManagers               0.740283   0.069897  10.591  &lt; 2e-16 ***\noccupationService-sales-workers -0.054056   0.067894  -0.796  0.42593    \noccupationProfessionals          0.404060   0.073506   5.497 3.86e-08 ***\noccupationTechnical              0.539975   0.103518   5.216 1.83e-07 ***\nraceWhite                        0.280506   0.053006   5.292 1.21e-07 ***\nsexMale                          0.313347   0.046955   6.673 2.50e-11 ***\nhours_per_week                   0.030166   0.001460  20.655  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 35948  on 32560  degrees of freedom\nResidual deviance: 23743  on 32536  degrees of freedom\nAIC: 23793\n\nNumber of Fisher Scoring iterations: 6\n\n\nThe model we’ve built above can be written algebraically as\n\\[\\ln(\\frac{\\rho}{1-\\rho}) = \\beta_{0} + \\beta_{1}X_{1} + \\cdots + \\beta_{p}X_{p}, \\]\nwhere \\(X_1, \\cdots, X_p\\) are the predictor variables, \\(β_0, \\cdots ,β_p\\) are the regression coefficients, \\(\\rho\\) is the probability of success, i.e., the probability of obtaining a particular value of the target variable, and \\(\\frac{\\rho}{1-\\rho}\\) is the odds for success.\nThe target, income, is a factor with two levels &lt;=50K and &gt;50K. R interprets the first level, &lt;=50K, as failure and the second level, &gt;50K, as success. For more information on how the glm() function handles the levels of factor variables, use the command ?glm to access the function’s documentation pages.\nFor a continuous predictor, the regression coefficient gives the change in the log odds for success when the predictor increases by 1 unit. We can also say that the regression coefficient is the log of the odds ratio comparing people who differ in that predictor by 1 unit, when the remaining predictors are held fixed (UCLA: Statistical Consulting Group, n.d.). For example, from the regression output above,\n\nA 1 unit increase in age should result in a 0.029 increase in the log odds of income being &gt;50K.\nA 1 unit increase in hours_per_week should result in a 0.030 increase in the log odds of income being &gt;50K.\n\nBy contrast, the regression coefficient for a level of a categorical predictor gives the log of the odds ratio comparing people at the given level to those at the base level, when other predictors are held fixed. So, it is crucial for the base level to be a meaningful quantity.\nFrom the regression output above, we can say that\n\nThe log of the odds ratio for income being &gt;50K comparing males to females is 0.313.\nThe log of the odds ratio for income being &gt;50K comparing whites to non-whites is 0.281.\nThe log of the odds ratio for income being &gt;50K comparing each level of education to the base level, Pre-school-12th, is greater than 1\n\nWe can also exponentiate the regression coefficients to give odd ratios. Here is the code:\n\nexp(coef(model1))\n\n                    (Intercept)                             age \n                   9.936811e-04                    1.029116e+00 \n         workclassOther/Missing                workclassPrivate \n                   2.046146e-01                    1.052772e+00 \n              workclassSelf-emp              educationAssociate \n                   8.170866e-01                    5.510721e+00 \n             educationBachelors              educationDoctorate \n                   1.012533e+01                    2.739002e+01 \n               educationHS-grad                educationMasters \n                   3.010897e+00                    1.454158e+01 \n           educationProf-school           educationSome-college \n                   2.818928e+01                    4.399996e+00 \n          marital_statusMarried     marital_statusNever-married \n                   7.321172e+00                    6.001186e-01 \n        marital_statusSeparated           marital_statusWidowed \n                   8.659304e-01                    9.820751e-01 \n        occupationOther/Missing          occupationLabor-trades \n                   2.396851e+00                    7.237814e-01 \n             occupationManagers occupationService-sales-workers \n                   2.096528e+00                    9.473788e-01 \n        occupationProfessionals             occupationTechnical \n                   1.497894e+00                    1.715964e+00 \n                      raceWhite                         sexMale \n                   1.323799e+00                    1.367996e+00 \n                 hours_per_week \n                   1.030626e+00 \n\n\nFrom the output above, we can now say that\n\nThe odds of income being &gt;50K increases by 1.029 when age increases by 1 unit.\nThe odds of income being &gt;50K increases by 1.031 when hours_per_week increases by 1 unit.\nThe odds ratio for income being &gt;50K comparing males to females is 1.368.\nThe odds ratio for income being &gt;50K comparing whites to non-whites is 1.324.\nThe odds ratio for income being &gt;50K comparing each level of education to the base level, Pre-school-12th, is greater than 1."
  },
  {
    "objectID": "posts/logistic-regression-adult-data.html#model-evaluation",
    "href": "posts/logistic-regression-adult-data.html#model-evaluation",
    "title": "Using logistic regression to predict income levels",
    "section": "Model Evaluation",
    "text": "Model Evaluation\nTask 10: Determine baseline accuracy\nWe set the baseline accuracy as the proportion of the majority class because a naïve model is highly likely to always predict this class.\n\nround(nrow(train_data[train_data$income == '&lt;=50K', ])/nrow(train_data), 2)\n\n[1] 0.76\n\n\nSo, the baseline accuracy score is 0.76. Because the dataset is slightly imbalanced, with approximately 76% of the data belonging to the “&lt;=50K” class, it’s better to use the balanced accuracy score to evaluate the performance of the logistic regression model. We explain how to calculate this and other evaluation metrics in Task 11.\nTask 11: Determine the training and test accuracies for the model.\nWe can use the predict() function to get the probability of an individual earning a &gt;50K income. If this probability is greater than 0.5, we assume that the individual earns a &gt;50K income. Otherwise, they earn a &lt;=50K income. We use the confusionMatrix() function from the caret package to compare the predicted income with the actual income.\n\nprob &lt;- predict(model1, newdata = train_data, type = \"response\")\npred_income &lt;- as.factor(ifelse(prob &gt; 0.5, \"&gt;50K\", \"&lt;=50K\"))\nconfusionMatrix(pred_income, train_data$income, positive = \"&gt;50K\")\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction &lt;=50K  &gt;50K\n     &lt;=50K 22861  3753\n     &gt;50K   1859  4088\n                                          \n               Accuracy : 0.8276          \n                 95% CI : (0.8235, 0.8317)\n    No Information Rate : 0.7592          \n    P-Value [Acc &gt; NIR] : &lt; 2.2e-16       \n                                          \n                  Kappa : 0.4863          \n                                          \n Mcnemar's Test P-Value : &lt; 2.2e-16       \n                                          \n            Sensitivity : 0.5214          \n            Specificity : 0.9248          \n         Pos Pred Value : 0.6874          \n         Neg Pred Value : 0.8590          \n             Prevalence : 0.2408          \n         Detection Rate : 0.1255          \n   Detection Prevalence : 0.1826          \n      Balanced Accuracy : 0.7231          \n                                          \n       'Positive' Class : &gt;50K            \n                                          \n\n\nFrom the output above, we see that the train accuracy (0.8276) is better than the previously computed baseline accuracy, called the No Information Rate (0.7592) in the output.\nOther important statistics generated by confusionMatrix() are defined below (Lee 2023):\n\nTrue positives (TP): the number of events the model correctly classifies as positive. In this case, TP = 4088.\nTrue negatives (TN): the number of events the model correctly classifies as negative. In this case, TN = 22861.\nFalse positives (FP): the number of events the model incorrectly classifies as positive. In this case, FP = 1859.\nFalse negatives (FN): the number of events the model incorrectly classifies as negative. In this case, FN = 3753.\nP-Value [Acc &gt; NIR]: the p value of a statistical test indicating whether the model’s accuracy is significantly better than the NIR.\nKappa (also called Cohen’s kappa): a measure of how well the model’s predictions agree with actual values. It ranges from -1 to 1. A less than 0 value suggests an agreement worse than chance, and a greater than 0 value suggests an agreement better than chance.\nMcnemar’s Test P-Value: the p value of a statistical test comparing the number of false positives to false negatives. A large p value suggests no significant difference between the two groups.\nSensitivity (also called recall or true positive rate): the proportion of actual positive events correctly classified by the model. It is calculated as \\(\\frac{TP}{TP + FN}\\).\nSpecificity (also called true negative rate): the proportion of actual negative events correctly classified by the model. It is calculated as \\(\\frac{TN}{FP + TN}\\)\nPositive predictive value (also called precision): the proportion of positive predictions that were true. It is calculated as \\(\\frac{TP}{TP + FP}\\).\nNegative Predictive Value: the proportion of negative predictions that were true. It is calculated as \\(\\frac{TN}{TN + FN}\\).\nPrevalence: the proportion of actually positive events in the dataset. It is the same as the “No Information Rate” also given in this output and calculated as \\(\\frac{TP + FN}{TP + FP + TN + FN}\\)\nDetection Rate: the proportion of true positives in the dataset. It is calculated as \\(\\frac{TP}{TP + FP + TN + FN}\\).\nDetection Prevalence: the proportion of events predicted as positive. It is calculated as \\(\\frac{TP + FP}{TP + FP + TN + FN}\\).\nBalanced Accuracy: the average of specificity and sensitivity.\n\nAs we’ve already mentioned, instead of using the standard accuracy score to evaluate the model’s performance, we can use the balanced accuracy, which is a more reliable measure of performance when dealing with imbalanced datasets. The balanced accuracy score for the baseline model is 0.5, while that for the logistic regression model is 0.7231. This result suggests that the model has learned meaningful patterns from the data and is not merely guessing the majority class.\nNext, we look at the model’s performance on the test data. But we must first import the data and transform it the same way we transformed the training data.\n\n# Read test data.\n# We skip the first line of the dataset (skip = 1) because it doesn’t\n# contain useful information.\ntest_data &lt;- read.csv(\"C:/Users/User/Desktop/Logistic_Regression_ADULT/Data/adult.test\", skip = 1, header = FALSE, sep = \",\", strip.white = TRUE, stringsAsFactors = TRUE)\n# Name columns\nnames(test_data) &lt;- c(\"age\", \"workclass\", \"fnlwgt\", \"education\", \"education_num\", \"marital_status\", \"occupation\", \"relationship\", \"race\", \"sex\", \"capital_gain\", \"capital_loss\", \"hours_per_week\", \"native_country\", \"income\")\n\nGet an overview of the test data:\n\nstr(test_data)\n\n'data.frame':   16281 obs. of  15 variables:\n $ age           : int  25 38 28 44 18 34 29 63 24 55 ...\n $ workclass     : Factor w/ 9 levels \"?\",\"Federal-gov\",..: 5 5 3 5 1 5 1 7 5 5 ...\n $ fnlwgt        : int  226802 89814 336951 160323 103497 198693 227026 104626 369667 104996 ...\n $ education     : Factor w/ 16 levels \"10th\",\"11th\",..: 2 12 8 16 16 1 12 15 16 6 ...\n $ education_num : int  7 9 12 10 10 6 9 15 10 4 ...\n $ marital_status: Factor w/ 7 levels \"Divorced\",\"Married-AF-spouse\",..: 5 3 3 3 5 5 5 3 5 3 ...\n $ occupation    : Factor w/ 15 levels \"?\",\"Adm-clerical\",..: 8 6 12 8 1 9 1 11 9 4 ...\n $ relationship  : Factor w/ 6 levels \"Husband\",\"Not-in-family\",..: 4 1 1 1 4 2 5 1 5 1 ...\n $ race          : Factor w/ 5 levels \"Amer-Indian-Eskimo\",..: 3 5 5 3 5 5 3 5 5 5 ...\n $ sex           : Factor w/ 2 levels \"Female\",\"Male\": 2 2 2 2 1 2 2 2 1 2 ...\n $ capital_gain  : int  0 0 0 7688 0 0 0 3103 0 0 ...\n $ capital_loss  : int  0 0 0 0 0 0 0 0 0 0 ...\n $ hours_per_week: int  40 50 40 40 30 30 40 32 40 10 ...\n $ native_country: Factor w/ 41 levels \"?\",\"Cambodia\",..: 39 39 39 39 39 39 39 39 39 39 ...\n $ income        : Factor w/ 2 levels \"&lt;=50K.\",\"&gt;50K.\": 1 1 2 2 1 1 1 2 1 1 ...\n\n\nThe dataset has 16281 observations and 15 variables. When we compare feature levels between the training and the test datasets, we see a match. Only the variables workclass, occupation, and native_country have missing values in both datasets, so we can use the same methods used to clean the training set to clean the test data. In addition, we should remove the dots in the test dataset’s income levels so they match the training dataset’s income levels.\nFirst, we get rid of all the variables we don’t need, i.e., capital_gain, capital_loss, native_country, education_num, fnlwgt, and relationship.\n\ntest_data$capital_gain &lt;- NULL\ntest_data$capital_loss &lt;- NULL\ntest_data$native_country &lt;- NULL \ntest_data$education_num &lt;- NULL\ntest_data$fnlwgt &lt;- NULL\ntest_data$relationship &lt;- NULL\n\nNext, we use the recode() function from the dplyr package to recode missing values and lump predictor levels.\nLump workclass levels\n\ntest_data$workclass &lt;- recode(test_data$workclass, \"?\" = \"Other/Missing\", \"Federal-gov\" = \"Gov\", \"Local-gov\" = \"Gov\", \"Never-worked\" = \"Other/Missing\", \"Self-emp-inc\" = \"Self-emp\", \"Self-emp-not-inc\" = \"Self-emp\", \"State-gov\" = \"Gov\", \"Without-pay\" = \"Other/Missing\")\n\nNext, we make “Gov” the base level of the variable.\n\ntest_data$workclass &lt;- relevel(test_data$workclass, ref = \"Gov\")\n\nLump education levels\n\ntest_data$education &lt;- recode(test_data$education, \"10th\" = \"Preschool-12th\", \"11th\" = \"Preschool-12th\", \"12th\" = \"Preschool-12th\", \"1st-4th\" = \"Preschool-12th\", \"5th-6th\" = \"Preschool-12th\", \"7th-8th\" = \"Preschool-12th\", \"9th\" = \"Preschool-12th\", \"Assoc-acdm\" = \"Associate\", \"Assoc-voc\" = \"Associate\", \"Preschool\" = \"Preschool-12th\")\n\nLump marital_status levels\n\ntest_data$marital_status &lt;- recode(test_data$marital_status, \"Married-AF-spouse\" = \"Married\", \"Married-civ-spouse\" = \"Married\", \"Married-spouse-absent\" = \"Married\")\n\nLump occupation levels\n\ntest_data$occupation &lt;- recode(test_data$occupation, \"Protective-serv\" = \"Service-sales-workers\", \"Sales\" = \"Service-sales-workers\", \"Priv-house-serv\" = \"Service-sales-workers\", \"Other-service\" = \"Service-sales-workers\", \"Exec-managerial\" = \"Managers\", \"Prof-specialty\" = \"Professionals\", \"Tech-support\" = \"Technical\", \"Adm-clerical\" = \"Clerical\", \"Machine-op-inspct\" = \"Labor-trades\", \"Handlers-cleaners\" = \"Labor-trades\", \"Farming-fishing\" = \"Labor-trades\", \"Craft-repair\" = \"Labor-trades\", \"Transport-moving\" = \"Labor-trades\", \"Armed-Forces\" = \"Other/Missing\", \"?\" = \"Other/Missing\")\n\nNext, we reorder occupation levels to make Clerical the base level.\n\ntest_data$occupation &lt;- relevel(test_data$occupation, ref = \"Clerical\")\n\nLump race levels\n\ntest_data$race &lt;- recode(test_data$race, \"Amer-Indian-Eskimo\" = \"Non-white\", \"Asian-Pac-Islander\" = \"Non-white\", \"Black\" = \"Non-white\", \"Other\" = \"Non-white\")\n\nClean income levels\nWe remove the dots in income level names, as shown below.\n\ntest_data$income &lt;- recode(test_data$income, \"&lt;=50K.\" = \"&lt;=50K\", \"&gt;50K.\" = \"&gt;50K\")\n\nHaving cleaned the test dataset, we can now use the predict() function to make predictions on the test set. We then use confusionMatrix() to compute the test accuracy score and other metrics.\n\nprob_test &lt;- predict(model1, newdata = test_data, type = \"response\")\npred_income_test &lt;- as.factor(ifelse(prob_test &gt; 0.5, \"&gt;50K\", \"&lt;=50K\"))\nconfusionMatrix(pred_income_test, test_data$income, positive = \"&gt;50K\")\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction &lt;=50K  &gt;50K\n     &lt;=50K 11498  1827\n     &gt;50K    937  2019\n                                         \n               Accuracy : 0.8302         \n                 95% CI : (0.8244, 0.836)\n    No Information Rate : 0.7638         \n    P-Value [Acc &gt; NIR] : &lt; 2.2e-16      \n                                         \n                  Kappa : 0.4887         \n                                         \n Mcnemar's Test P-Value : &lt; 2.2e-16      \n                                         \n            Sensitivity : 0.5250         \n            Specificity : 0.9246         \n         Pos Pred Value : 0.6830         \n         Neg Pred Value : 0.8629         \n             Prevalence : 0.2362         \n         Detection Rate : 0.1240         \n   Detection Prevalence : 0.1816         \n      Balanced Accuracy : 0.7248         \n                                         \n       'Positive' Class : &gt;50K           \n                                         \n\n\nFrom the output above, we see that the test accuracy is 0.83, which is slightly better than the train accuracy. Also, the test balanced accuracy (0.7248) is slightly higher than the training balanced accuracy (0.7231). So, we can conclude that the model probably generalizes well on new data."
  }
]